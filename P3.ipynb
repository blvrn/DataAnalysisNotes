{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47dff7d2-6353-478c-b39d-9ba83d7b779b",
   "metadata": {},
   "source": [
    "# MAT 388, HW4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ee240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras as ks\n",
    "import urllib.request\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, LSTM, Embedding, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,  classification_report\n",
    "from sklearn.datasets import load_iris, load_digits, fetch_20newsgroups_vectorized, fetch_olivetti_faces\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35472d1b-55e1-444c-99ef-74b17f961e7b",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "For this question we are goint to use [CELEB_A](https://www.tensorflow.org/datasets/catalog/celeb_a) dataset through [tensorflow datasets](https://www.tensorflow.org/datasets).\n",
    "\n",
    "1. Ingest the data, and select 10000 images from the dataset. Put the images under a variable called `X` and class labels (attributes) into `y`.\n",
    "2. Build a neural network model for `X` against `Glasses` attribute in `y`.\n",
    "3. Test the accuracy of your model using a 5-fold cross-validation. (I want a %95 confidence interval on the returned result using a t-test as I did in one of my lectures.)\n",
    "4. Repeat Steps 2 and 3 for `Male` and `Bangs` attributes.\n",
    "5. Compare your results for `Glasses`, `Male` and `Bangs`. Which one is better? Why? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273a426",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the CELEB_A dataset\n",
    "dataset, info = tfds.load('celeb_a', with_info=True)\n",
    "\n",
    "# Select the training split and only keep the Glasses attribute\n",
    "train_dataset = dataset['train'].map(lambda x: (x['image'], x['attributes']['Eyeglasses']))\n",
    "\n",
    "# Select the first 10000 examples from the training split\n",
    "X, y = [], []\n",
    "for example in train_dataset.take(10000):\n",
    "  image, label = example\n",
    "  X.append(image)\n",
    "  y.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c5a9a",
   "metadata": {},
   "source": [
    "# 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c90ae1",
   "metadata": {},
   "source": [
    "# 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8475f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import t\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "accuracy = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    model = create_model()\n",
    "    model.fit(X[train_index], y[train_index], epochs=10, verbose=0)\n",
    "    score = model.evaluate(X[test_index], y[test_index], verbose=0)\n",
    "    scores.append(accuracy[1])\n",
    "\n",
    "print(\"Cross-validation scores:\", accuracy)\n",
    "print(\"Mean accuracy:\", np.mean(accuracy))\n",
    "print(\"Standard deviation:\", np.std(accuracy))\n",
    "\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "confidence = 0.95\n",
    "n = len(accuracy)\n",
    "mean = np.mean(accuracy)\n",
    "std = np.std(accuracy)\n",
    "\n",
    "interval = stats.t.interval(confidence, n-1, loc=mean, scale=std/np.sqrt(n))\n",
    "print(\"Confidence interval:\", interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcc146",
   "metadata": {},
   "source": [
    "# 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3204bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the attributes to predict\n",
    "attributes = ['Male', 'Bangs']\n",
    "\n",
    "# Loop over the attributes\n",
    "for attribute in attributes:\n",
    "  # Select the training split and only keep the current attribute\n",
    "  train_dataset = dataset['train'].map(lambda x: (x['image'], x['attributes'][attribute]))\n",
    "\n",
    "  # Select the first 10000 examples from the training split\n",
    "  X, y = [], []\n",
    "  for example in train_dataset.take(10000):\n",
    "    image, label = example\n",
    "    X.append(image)\n",
    "    y.append(label)\n",
    "\n",
    "  # Flatten the images and add a fully connected layer\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(218, 178, 3)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  # Compile the model with a binary cross-entropy loss and an Adam optimizer\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  # Define the KFold cross-validator\n",
    "  kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "  # Initialize a list to store the scores for each fold\n",
    "  scores = []\n",
    "\n",
    "  # Loop over the folds\n",
    "  for train_index, test_index in kfold.split(X):\n",
    "    # Split the data into train and test sets for the current fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model on the train set and evaluate on the test set\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    scores.append(score[1])\n",
    "\n",
    "  # Calculate the mean and standard deviation of the scores\n",
    "  mean = np.mean(scores)\n",
    "  std = np.std(scores)\n",
    "\n",
    "  # Calculate the t-value and degrees of freedom\n",
    "  t_value = t.ppf(0.975, len(scores) - 1)\n",
    "\n",
    "  # Calculate the confidence interval\n",
    "  confidence_interval = t_value * std / np.sqrt(len(scores))\n",
    "\n",
    "  print(f\"Attribute: {attribute}\")\n",
    "  print(f\"Mean accuracy: {mean:.4f}\")\n",
    "  print(f\"Confidence interval: {mean - confidence_interval:.4f} - {mean + confidence_interval:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a1987",
   "metadata": {},
   "source": [
    "# 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3e253",
   "metadata": {},
   "source": [
    "The attribute with the highest mean accuracy and a confidence interval that does not overlap with the others is generally considered the best. In our model, the accuracy values for \"Eyeglasses\",\"Male\" and \"Bangs\" were calculated as 93.54%, 61.35% and 84.43%, respectively. So the \"Eyeglasses\" attribute is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896ee2e-06ab-40ba-b2bc-3f213b7410d4",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "For this question use the [Hyperspectral Image of Kennedy Space Center](https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Kennedy_Space_Center_.28KSC.29).\n",
    "\n",
    "1. Ingest the image data directly from the web. (No local files!)\n",
    "2. Ingest the ground truth data directly from the web. (No local files!)\n",
    "4. Build a convolutional neural network (preferably using [this](https://keras.io/api/layers/convolution_layers/) and/or [this](https://keras.io/api/layers/recurrent_layers/conv_lstm2d/)) model.\n",
    "5. Test the accuracy of the model using a 5-fold cross-validation. (I want a %95 confidence interval on the returned result using a t-test as I did in one of my lectures.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d6db7",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "ksc_url = \"https://www.ehu.eus/ccwintco/uploads/2/26/KSC.mat\"\n",
    "X=urllib.request.urlretrieve(ksc_url,\"ksc_data.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ea49d",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "ksc_gt_url = \"https://www.ehu.eus/ccwintco/uploads/a/a6/KSC_gt.mat\"\n",
    "Y=urllib.request.urlretrieve(ksc_gt_url,\"ksc_gt_data.mat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e1fda",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[0],Y[0],train_size=0.2)\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(8,8,3)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Add pooling layers\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KerasClassifier\n",
    "model = KerasClassifier(build_fn=build_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Create the cross-validation object\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0.2)\n",
    "\n",
    "# Use cross_val_score to evaluate the model\n",
    "scores = cross_val_score(model, x_train, y_train, cv=kfold)\n",
    "\n",
    "# Print the mean and standard deviation of the scores\n",
    "print(f\"Mean accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741f555",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eb75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(scores)\n",
    "scores = np.array(scores)\n",
    "mean = scores.mean()\n",
    "std = scores.std()\n",
    "ci = t.interval(0.95, n - 1, loc=mean, scale=std / np.sqrt(n))\n",
    "\n",
    "print(f\"95% confidence interval: {ci[0]:.4f} - {ci[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1299167-dc7b-4a54-ae1b-5c3bd5c40c73",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "For this question we are going to use the time series of [Wheat Futures](https://finance.yahoo.com/quote/ZW=F/) from Yahoo Finance.\n",
    "\n",
    "1. Ingest the data using [yfinance](https://pypi.org/project/yfinance/) Start from Jan 1, 2010 until Dec 31, 2021.\n",
    "2. Construct [a RNN and/or a LSTM](https://keras.io/api/layers/recurrent_layers/) model on the data.\n",
    "3. Test your model on the wheat futures data from Jan 1, 2022 to today using a 5-fold cross-validation. Did your model work? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370527dc",
   "metadata": {},
   "source": [
    "# 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "\n",
    "# Download the data\n",
    "data = yf.download(\"ZW=F\",start='2010-01-01', end='2021-12-31')\n",
    "data.dropna(inplace=True)\n",
    "# Print the data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f77215",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if data['Close'].values.shape[0] == 0:\n",
    "    print(\"Data is empty\")\n",
    "elif data['Close'].values.shape[0] == 1:\n",
    "    print(\"Data has only one value\")\n",
    "else:\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "if data['Close'].values.shape[0] == 0:\n",
    "    X_train, X_test, y_train, y_test = (np.array([]), np.array([]), np.array([]), np.array([]))\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_data[:-1], scaled_data[1:], test_size=0.2)\n",
    "\n",
    "# Convert the data to a 3D format suitable for an LSTM model\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=32, input_shape=(1, 1)))\n",
    "model.add(Dense(1))\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', run_eagerly=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "predictions = model.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5ba53",
   "metadata": {},
   "source": [
    "# 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362023e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if data.shape[0] == 0:\n",
    "    print(\"Data is empty\")\n",
    "else:\n",
    "    # Define the cross-validation split\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize the list of evaluation scores\n",
    "scores = []\n",
    "\n",
    "# Iterate over the folds\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "    \n",
    "    # Evaluate the model on the testing set\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Append the score to the list\n",
    "    scores.append(score)\n",
    "\n",
    "# Print the evaluation scores\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd3167-d198-412b-a794-29e887e2250b",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "For this question, we are going to use [Consumer Complaints Dataset](https://raw.githubusercontent.com/plotly/datasets/master/26k-consumer-complaints.csv).\n",
    "\n",
    "1. Ingest the dataset. We are only going to use the columns `Issue` and `Timely Response?`.\n",
    "2. Convert the values in the issue column to vectors using [Count Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from `scikit-learn`. Similarly, binarize the column `Timely Response` using [Label Binarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html).\n",
    "3. Split your dataset into train and test.\n",
    "3. Construct an an appropriate neural network model on the train set.\n",
    "4. Test your model on the test set. Did your model work? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da6f5f",
   "metadata": {},
   "source": [
    "# 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset from a URL\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/26k-consumer-complaints.csv')\n",
    "\n",
    "# Select the \"Issue\" and \"Timely Response?\" columns\n",
    "df = df[['Issue', 'Timely response?']]\n",
    "issue = df[['Issue']]\n",
    "time = df[['Timely response?']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5de97",
   "metadata": {},
   "source": [
    "# 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(issue)\n",
    "\n",
    "# Binarize the \"Timely response?\" column\n",
    "binarizer = LabelBinarizer()\n",
    "Y = binarizer.fit_transform(time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48192428",
   "metadata": {},
   "source": [
    "# 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02006be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Issue'].values\n",
    "Y = df['Timely response?'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a37c7",
   "metadata": {},
   "source": [
    "# 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0916f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d36a4d2",
   "metadata": {},
   "source": [
    "# 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa17741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb480d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8c058c4299ec31067a796c6d031d2ea1f4c7c1fdb07246e19fa4a966639df14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
